{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Exploratory data analysis\n",
    "## Group 8\n",
    "#### Godun Alina, B√∂rtlein Lorenz, Kodar Mark-Eerik, Car Franko\n",
    "### Algorithm - P3C\n",
    "### Dataset - ENZYMES\n",
    "\n",
    "Our choice of dataset has been the enzymes dataset. Enzymes play critical role in living organisms.\n",
    "Understanding enzymes, predicting their structure and functionality portrays an integral challenge in modern science.\n",
    "An enzyme can be regarded in four different structural levels, which are all important to determine the certain functionality and application.\n",
    "An enzyme consists of a certain amino acid sequence. This depicts the primary structure of an enzyme.\n",
    "The secondary structure displays the spacial alignment of the protein, largely due to hydrogen bonds that exert a force of attraction onto the other enzymes.\n",
    "Detecting these folds with a high accuracy is a well known problem in science and the solving of that problem took a huge step forward by a new AI guided approach of Google's DeepMind.\n",
    "When having attained the knowledge of the amino acid sequence and the secondary structural properties that make up an enzyme, one can compare this information to a\n",
    "wide range of different databases according to different properties of the enzyme. Thus, when finding a certain threshold of similarity according to a distinct feature\n",
    "the functional properties of an enzyme can be inferred. Sadly, this technique is lacking a universal validity, as the comparison of structural properties merely may infer a\n",
    "common ancestor, thus concluding same properties. So, Borgwardt et al. developed a graph guided approach using graph kernels and support vector machine classification to represent\n",
    "several types of information defining different similarity measures of an enzyme according to different protein function prediction systems, thus making the evaluation more reliable.\n",
    "To reduce the loss of information when applying SVM to a dataset with high dimensionality, they represent protein structures as graphs of their secondary structure elements, the folding of the enzyme.\n",
    "Our goal is to cluster the given data according to the interval approach guided clustering algorithm P3C to gain insight into the different properties of our enzyme.\n",
    "After clustering, we assert the found cluster cores with their assigned points to the according graph nodes of our trained model.\n",
    "In the next step we will be able to perform graph-matching algorithms to measure structural similarity between the different enzymes to infer shared properties.\n",
    "The goal is to conclude the correct enzyme commission number (EC), which is a numerical classification for enzyme-catalyzed reactions, of a given enzyme according to a probabilistic decision.\n",
    "\n",
    "### P3C (Projected Clustering via Cluster Cores)\n",
    "\n",
    "P3C has the following properties:\n",
    ">- Effectively discovers the projected clusters in the data while being remarkable robust to the only parameter that it takes as input. The setting of this parameter requires little prior knowledge about the data and, in contrast to all previous approaches, there is no need to provide the number of projected clusters as input, since our algorithm can discover, under very general conditions, the true number of projected clusters.\n",
    ">- Effectively discovers very low-dimensional projected clusters embedded in high-dimensional spaces.\n",
    ">- Effectively discovers clusters with varying orientation in their relevant subspaces.\n",
    ">- Scalable with respect to large data sets and high number of dimensions. \n",
    "\n",
    "The algorithm is comprised of the following steps:\n",
    ">- Regions corresponding to projections of clusters onto single attributes are computed.\n",
    ">- Cluster cores are identified by spatial areas that (1) are described by combination of the detected regions and (2) contain an unexpectedly large number of points.\n",
    ">- Cluster cores are iteratively refined into projected clusters\n",
    ">- Outliers are identified and the relevant attributes for each cluster are determined.\n",
    "\n",
    "### Introduction\n",
    "\n",
    "We chose the enyzmes dataset as one of our team members had prior experience with the field and it seemed interesting to the other participants as well. After that we proceeded with the algorithm selection and thought that P3C might suit our dataset due to the fact that it would be scalable with high number of dimensions and would be relatively easy to implement as it required tuning of only one parameter. In addition to that, the original paper stated that it is faster than some of the other algorithms we could choose (e.g. PROCLUS, ORCLUS)\n",
    "\n",
    "In reality the latter the part about the parameters is not true as there are more parameters that influence the performance of the algorithm: computation for the number of bins, degrees of freedom for noise prediction.\n",
    "In addition to that we had two team members leave the project one of which was the member with the knowledge of enzymes. This meant that further development was influenced by that and it made the evaluation of our results harder. \n",
    "\n",
    "### P3C implementation steps\n",
    "\n",
    "The implementation has the following steps:\n",
    "- Projections of true $p$-signatures\n",
    "    - <u>Idea</u>: For each attribute compute the intervals that match or approximate well the projections of true $p$-signatures onto that attribute.\n",
    "    - Identify attributes with uniform distribution and for the non-uniform attributes, to identify intervals with unusual high support using Chi-square goodness-of-fit test.\n",
    "    - Each attribute is divided into same number of equi-sized bins. For every bin in every attribute, its support is computed.\n",
    "    - On the attributes deemed non-uniform, the bin with the largest support is marked. The remaining un-marked bins are tested again using the Chi-square test for uniform distribution. If the Chi-square test indicates that the un-marked bins \"look\" uniform, then we stop. Otherwise, the bin with the second-largest support is marked. Repeat testing for remaining un-marked bins for the uniform distribution and marking bins in decreasing order of support, until the current set of un-marked bins satisifies the test. \n",
    "    - Compute intervals by merging adjacent marked bins. This marking process of bins is linear in the number of bins. \n",
    "- Finding the cluster cores\n",
    "    - <u>Idea</u>: Determining which calculated $p$-signatures do in fact represent the projected clusters.\n",
    "        - <u>Idea</u>: Compute the expected support of a given set when extending it by an additional attribute, while assuming uniform distribution.\n",
    "        - Compare the expected support to the actual support of the candidate p-signature\n",
    "        - Assumption: If the actual support greatly exceeds the expected one it is highly likely that the true projected cluster (true ùë°-signature) expands over the newly added attribute\n",
    "        - Validate according to the value of the Poission Probability density function how likely the observation was\n",
    "    - Repeat until maximal sets are being found\n",
    "    - Compute how many points are expected to belong to the support set to a specific support set in the case when they are not part of the true $t$-signature.\n",
    "    - Compute the expected support of the signatures.\n",
    "    - Check whether a support set belongs to the true $t$-signature by comparing the expected support to the actual support with the help of Poission probability density function. \n",
    "    - The results of that are considered to be the cluster cores.\n",
    "- Computing the projected clusters\n",
    "    - <u>Idea</u>: The support sets of the cluster cores found in the previous point may not necessarily contain all and only the points of the projected clusters that the cluster cores approximate, depending on the accuracy of the intervals computed in the first step. Refine found $k$ cluster cores into $k$ projected clusters. This is performed in a subscape of (reduced) dimensionality $d'$ of the original $d$-dimensional data, containing all attributes that were deemed non-uniform.\n",
    "    - Describe the membership of data points to cluster cores through a fuzzy membership matrix $M=(m_{il})_{i=1,n,l,=1,k}$, where $m_{il}$ denotes the membership of object $i$ to cluster core $l$; it is defined as follows: $m_{il}=0$ if data point $i$ does not belong to the support set of any cluster core; $m_{il}$ is equal to the fraction of cluster cores that contain data point $i$ in their support set, if $i$ is in the support set of cluster core $l$. \n",
    "    - Compute the probability of a data point belonging to each projected cluster using Expectation Maximization algorithm which is initialized with the fuzzy membership matrix $M$. \n",
    "    - Assign points that have value 0 in the fuzzy membership matrix to one of the cluster cores with the shortest Mahalanobis distance to the cluster mean.\n",
    "    - EM returns the matrix of probabilities that gives for each data point its probability of belonging to each projected cluster and assign each data point to the most probable cluster.\n",
    "- Detect outliers\n",
    "    - <u>Idea</u>: Find points that should not be a part of any cluster.\n",
    "    - Use standard technique for multivariate outlier detection: The Mahalanobis distances between data points and the means of the projected clusters to which they belong are compared to the critical value of the Chi-square distribution with $d'$ degrees of freedom at a confidence level of $\\alpha=0.001$.\n",
    "    - Data points with Mahalanobis distances larger than this critical value are declared outliers. \n",
    "- Detect relevant attributes\n",
    "    - <u>Idea</u>: Determine the relevant attributes of each projected cluster based on the cluster members.\n",
    "    - The relevant attributes of a projected cluster include the attributes of the intervals that make up the $p$-signature of the cluster core based on which this cluster has been computed. \n",
    "    - Test each projected cluster using the Chi-square test whether its members are uniformly distributed in the attributes initially deemed uniform. \n",
    "    - Members of a projected cluster that are not uniformly distributed in one of the attributes initially considered uniform are considered to be relevant for the projected cluster.\n",
    "    - The $p$-signatures of projected clusters are refined by computing the smallest interval that the cluster members project to for each relevant attribute. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example visualization of the algorithm\n",
    "\n",
    "#### First, we will generate a simple dataset with clear clusters to aid with visualisation\n",
    "\n",
    "<img src=\"sample_dataset1.png\">\n",
    "<img src=\"sample_dataset2.png\">\n",
    "\n",
    "#### Then we split the data into bins along each axis.\n",
    "\n",
    "##### Dimension 0\n",
    "<img src=\"bins0.png\">\n",
    "\n",
    "##### Dimension 1\n",
    "<img src=\"bins1.png\">\n",
    "\n",
    "##### Dimension 2\n",
    "<img src=\"bins2.png\">\n",
    "\n",
    "#### Put together\n",
    "\n",
    "<img src=\"bins.png\">\n",
    "\n",
    "#### We merge the bins\n",
    "\n",
    "##### Dimension 0\n",
    "<img src=\"merged_bin0.png\">\n",
    "\n",
    "##### Dimension 1\n",
    "<img src=\"merged_bin1.png\">\n",
    "\n",
    "##### Dimension 2\n",
    "<img src=\"merged_bin2.png\">\n",
    "\n",
    "<img src=\"merged_bins_lines.png\">\n",
    "\n",
    "<img src=\"merged_bins3d.png\">\n",
    "\n",
    "#### Candidates\n",
    "\n",
    "<img src=\"candidates3d.png\">\n",
    "\n",
    "#### After outlier detection\n",
    "\n",
    "<img src=\"outliers.png\">\n",
    "\n",
    "<img src=\"outliers3d.png\">\n",
    "\n",
    "\n",
    "#### Play around with the visualization in the file <code>AlgoVisualisation.ipynb</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Our implementation results\n",
    "After implementing the algorithm we evaluated it by comparing its clustering results with the results of the ELKI implementation. For the implementation comparison, we have used two datasets provided by ELKI: mouse and vary density. To verify different steps of the algorithm during the implementation, we have also generated a custom synthetic dataset consisting of multiple well-separated uniformly distributed clusters.\n",
    "\n",
    "As we can see, PC3 does not work well for both mouse and vary density datasets in both our and ELKI‚Äôs implementations. The reason for this is that the algorithm heavily relies on the bins provided in the first step, and in both datasets, the points are not well separated in either of the dimensions, so PC3 finds only one big bin in each of the dimensions.\n",
    "\n",
    "#### P3C ELKI result for mouse dataset\n",
    "\n",
    "<img src=\"ELKI_MOUSE_DS.PNG\">\n",
    "\n",
    "#### P3C our implementation result for mouse dataset\n",
    "<img src=\"ours_mouse_DS.PNG\">\n",
    "\n",
    "#### P3C ELKI result for vary dataset\n",
    "<img src=\"ELKI_vary_DS.PNG\">\n",
    "\n",
    "#### P3C our implementation result for vary dataset\n",
    "<img src=\"ours_vary_DS.PNG\">\n",
    "\n",
    "The algorithm works much better on the custom dataset, where the clusters are well separated in each of the dimensions, and subsequent cluster core search successfully identifies bins intersections in multiple dimensions. The ELKI implementation found 2 clusters and we found 3. It seems that ELKI implementation allows for each dimension to have only one correspondence in the another dimension, that's why when one bin in the x dimension corresponds to multiple bins in the other dimension, it merges two different bins into one cluster core. The paper does not state how this problem should be solved directly, so we assumed that one bin in one dimension can correspond to multiple bins in other dimensions, thus creating multiple candidates for the cluster cores. As we see, in this case out implementation provides a better result.\n",
    "\n",
    "#### P3C ELKI result for synthetic dataset\n",
    "<img src=\"ELKI_synthetic_DS.PNG\">\n",
    "\n",
    "#### P3C our implementation result for synthetic dataset\n",
    "<img src=\"ours_synthetic_DS.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running our implementation on the enzymes dataset\n",
    "\n",
    "Unfortunately the cluster core candidate generation step is very expensive, since we try to find intersections between all bins in all dimensions. Even with apriori-like approach, where higher dimension cluster core candidate generation is performed by merging valid intersections of smaller number of dimensions, the number of bins which should be compared is enormous. Thus running algorithm on the whole gram matrixes is not possible. To omit this problem, we have run our algorithm on the KPCA reduced number of dimensions. \n",
    "\n",
    "We have tried different number of dimensions and have decided for the 10 dimensions for PCA. The bigger number of dimensions did not provide any sensible results, probably because with additional dimensions the cluster cores were getting smaller and less representative. \n",
    "\n",
    "The graph shows clustering results on the 10-KPCA data. The red dots represent cluster centers before the final step when only cluster cores are considered, the green dots represent cluster centers after all points are assigned to some clusters and thus cluster centers are recomputed.\n",
    "<img src=\"ours_enzymes_10d.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
