{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the csv, ignores lines with comments.\n",
    "The result is a simple python array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data = []\n",
    "with open('enzymes_5.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    for row in reader:\n",
    "        if len(row) > 0 and not row[0].startswith('#'):\n",
    "            data.append([float(r) for r in row])\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def normalize_dataset(data):\n",
    "    for column_id in range(len(data[0])):\n",
    "        column_max = max([x[column_id] for x in data])\n",
    "        for i, entry in enumerate([x[column_id] for x in data]):\n",
    "             data[i][column_id] = entry/column_max\n",
    "    return data\n",
    "#data = normalize_dataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#for d in data:\n",
    "#    print(d[0], d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare,chi2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "import scipy as sp\n",
    "import sys\n",
    "from sklearn import mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#data = np.random.uniform(size=[50000, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "centers = [(0.2, 0.2), (0.2, 0.8), (0.5, 0.5)]\n",
    "cluster_std = [0.05, 0.05, 0.05]\n",
    "\n",
    "#data, y = make_blobs(n_samples=1000, cluster_std=cluster_std, centers=centers, n_features=3, random_state=1)\n",
    "\n",
    "#plt.scatter(data[y == 0, 0], data[y == 0, 1], color=\"red\", s=10, label=\"Cluster1\")\n",
    "#plt.scatter(data[y == 1, 0], data[y == 1, 1], color=\"blue\", s=10, label=\"Cluster2\")\n",
    "#plt.scatter(data[y == 2, 0], data[y == 2, 1], color=\"green\", s=10, label=\"Cluster3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import csv\n",
    "\n",
    "with open('synthetic.csv', mode='w') as employee_file:\n",
    "    employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for l in data:\n",
    "        employee_writer.writerow(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class to store data about bins.\n",
    "Makes it easier to keep track of marked bins and merging them.\n",
    "\n",
    "Holds the index of the bin in the initial array, mapping to the dataset.\n",
    "\n",
    "Holds an array of all subsequent bins that should be merged to itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create datapoint class\n",
    "\n",
    "class Bin:\n",
    "    def __init__(self, index, interval, dim):\n",
    "        self.interval = interval\n",
    "        self.marked = False\n",
    "        self.support = 0\n",
    "        self.index = index\n",
    "        self.merge_with = []\n",
    "        self.assigned_points = []\n",
    "        self.dimension = dim\n",
    "        self.id = 'd{}b{}'.format(dim, index)\n",
    "        \n",
    "    def add_point(self, point):\n",
    "        self.support += 1\n",
    "        self.assigned_points.append(point)\n",
    "        \n",
    "    def get_width(self):\n",
    "        return self.interval.length\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Interval: {0}; Marked: {1}; Support: {2}; Index: {3}; \\\n",
    "            Merge-with {4}; # of assigned points {5}; Dimension: {6}'.format(\n",
    "            self.interval,\n",
    "            self.marked,\n",
    "            self.support,\n",
    "            self.index,\n",
    "            self.merge_with,\n",
    "            len(self.assigned_points),\n",
    "            self.dimension\n",
    "        )\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if type(self) != type(other):\n",
    "            return False\n",
    "        return self.id == other.id\n",
    "    \n",
    "class Interval:\n",
    "    def __init__(self, start, end):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.length = end - start\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Start: {0}; End: {1}; Length {2}'.format(self.start, self.end, self.length)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the interval, splits the data into bins.\n",
    "TODO: Works on one dimension only, should be generalised to an arbitrary number of dimensions according to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_bins(data):\n",
    "    bins = []\n",
    "    for col_idx in range(len(data[0])):\n",
    "        column_bins = []\n",
    "        column_data = [x[col_idx] for x in data]\n",
    "        col_min = min(column_data)\n",
    "        col_max = max(column_data)\n",
    "\n",
    "        interval_length = col_max - col_min\n",
    "        # divide interval 1 + log2(n) bins\n",
    "        nr_of_bins = math.floor(1 + math.log2(len(column_data)))\n",
    "        \n",
    "        column_bins = []\n",
    "        b = interval_length / nr_of_bins\n",
    "        for i in range(nr_of_bins):\n",
    "            # adds a bin with a start and end interval\n",
    "            bin_interval = Interval(col_min + b * i, col_min + b * (i + 1))\n",
    "            \n",
    "            column_bins.append(Bin(i, bin_interval, col_idx))\n",
    "\n",
    "        # compute support for each bin\n",
    "        for i, datapoint in enumerate(column_data):\n",
    "            bin_index = int((datapoint - col_min) / b)\n",
    "            bin_index = bin_index if bin_index < nr_of_bins else bin_index - 1\n",
    "            column_bins[bin_index].add_point(data[i])\n",
    "\n",
    "        bins.append(column_bins)\n",
    "    return bins, nr_of_bins\n",
    "\n",
    "bins, nr_of_bins = split_into_bins(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter([x[0] for x in data], [y[1] for y in data])\n",
    "#plt.scatter([b.interval[0] for b in bins[0]], [0.5 for _ in range(len(bins[0]))], c='green', s=100)\n",
    "#plt.scatter([b.interval[1] for b in bins[0]], [0.5 for _ in range(len(bins[0]))], c='red')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"On the attributes deemed non-uniform, the bin with the largest support is marked. The remaining un-marked bins are tested again using the Chi-square test for uniform distribution. If the Chi-square test indicates that the un-marked bins “look” uniform, then we stop. Otherwise, the bin with the second-largest support is marked. Then, we repeat testing the remaining un-marked bins for the uniform distribution and marking bins in decreasing order of support, until the current set of un-marked bins satisfies the Chi-square test for uniform distribution.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the bin with the highest support and mark it\n",
    "def mark_highest_support(column_bins):\n",
    "    max_support = 0\n",
    "    max_index = 0\n",
    "    for _bin in column_bins:\n",
    "        if _bin.marked:\n",
    "            continue\n",
    "        if _bin.support > max_support:\n",
    "            max_support = _bin.support\n",
    "            max_index = _bin.index\n",
    "    #print(max_support, max_index)\n",
    "    column_bins[max_index].marked = True\n",
    "\n",
    "# perform chisquared for the support \n",
    "def mark_bins(column_bins, alpha=0.001, stat=1):\n",
    "    while (stat > alpha):\n",
    "        # support list of all *unmarked* bins\n",
    "        support_list = [column_bins[i].support for i in range(nr_of_bins) if not column_bins[i].marked]\n",
    "        #print(support_list)\n",
    "        # if there are no unmarked bins, end the process\n",
    "        if len(support_list) == 0: \n",
    "            break\n",
    "        (stat, p) = chisquare(support_list)\n",
    "        #print('stat', stat)\n",
    "        #print('p', p)\n",
    "        if (stat > alpha):\n",
    "            mark_highest_support(column_bins)\n",
    "            \n",
    "\n",
    "    #print(list(column_bins[i].support for i in range(nr_of_bins) if not column_bins[i].marked))\n",
    "    \n",
    "for column_bins in bins:\n",
    "    mark_bins(column_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the adjacent bins that have the same marked status. Add all following bins with the same status to `merge_with` array in the Bin objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge bins\n",
    "def mark_merge_bins(column_bins):\n",
    "    for i, _bin1 in enumerate(column_bins):\n",
    "        for j, _bin2 in enumerate(column_bins[i+1:]):\n",
    "            if _bin1.marked == _bin2.marked:\n",
    "                _bin1.merge_with.append(_bin2.index)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "for column_bins in bins:\n",
    "    mark_merge_bins(column_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merges the bins according to the merge list. New bins in a new array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge each bin in the list by extending the interval and combining support\n",
    "def merge_bin(all_column_bins, column_bin):\n",
    "    for bin_index in column_bin.merge_with:\n",
    "            column_bin.interval.end = all_column_bins[bin_index].interval.end\n",
    "            column_bin.support += all_column_bins[bin_index].support\n",
    "            column_bin.assigned_points.extend(all_column_bins[bin_index].assigned_points)\n",
    "\n",
    "# merge bins of a single column\n",
    "def merge_column_bins(column_bins):\n",
    "    i = 0\n",
    "    new_bins = []\n",
    "    while i < len(column_bins):\n",
    "        # if bin has no following bins to merge with, keep it as is, and go to the next one\n",
    "        if len(column_bins[i].merge_with) == 0:\n",
    "            new_bins.append(column_bins[i])\n",
    "            i += 1\n",
    "            continue\n",
    "            \n",
    "        merge_bin(column_bins, column_bins[i])\n",
    "        \n",
    "        new_bins.append(column_bins[i])\n",
    "        \n",
    "        # skip all of the bins that were included in the current one\n",
    "        i = max(column_bins[i].merge_with) + 1\n",
    "        \n",
    "    return new_bins\n",
    "\n",
    "def merge_all_bins(bins):\n",
    "    new_bins = []\n",
    "    for column_bins in bins:\n",
    "        new_bins.append(merge_column_bins(column_bins))\n",
    "    return new_bins\n",
    "\n",
    "new_bins = merge_all_bins(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim1 = 17\n",
    "dim2 = 22\n",
    "\n",
    "dims = [i for i, x in enumerate(new_bins) if len(x) > 3]\n",
    "\n",
    "for i in range(0, len(dims)-1, 2):\n",
    "    dim1 = dims[i]\n",
    "    dim2 = dims[i + 1]\n",
    "    plt.scatter([x[dim1] for x in data], [y[dim2] for y in data])\n",
    "    plt.scatter([b.interval.start for b in new_bins[dim1]], [0.5 for _ in range(len(new_bins[dim1]))], c='green', s=100)\n",
    "    plt.scatter([b.interval.end for b in new_bins[dim2]], [0.5 for _ in range(len(new_bins[dim2]))], c='red')\n",
    "\n",
    "    plt.scatter([0.5 for _ in range(len(new_bins[dim1]))], [b.interval.start for b in new_bins[dim1]], c='yellow', s=100)\n",
    "    plt.scatter([0.5 for _ in range(len(new_bins[dim2]))], [b.interval.end for b in new_bins[dim2]], c='purple')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x.support for x in new_bins[0]) # confirm all points are kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "\n",
    "def create_new_candidate(candidate, dim_bin, reevaluated_points):\n",
    "    current_bins_list = []\n",
    "    current_bins_list.extend(candidate.bins)\n",
    "    current_bins_list.append(dim_bin)\n",
    "    return PSignature(current_bins_list, reevaluated_points)\n",
    "\n",
    "def generate_candidate_list(candidate_list, current_dim_bins, threshold, current_dim):\n",
    "    new_candidates = []\n",
    "    for candidate in candidate_list:\n",
    "        for dim_bin in current_dim_bins:\n",
    "            if dim_bin.marked:\n",
    "                expected_sup = candidate.get_support() * dim_bin.get_width()            \n",
    "                reevaluated_points = candidate.reevaluate_assigned_points(dim_bin, current_dim)\n",
    "                r_support = len(reevaluated_points)\n",
    "                if r_support == 0:\n",
    "                    continue\n",
    "                print('R support {0}, expected support {1}'.format(r_support, expected_sup))\n",
    "                print('Poisson distribution:', poisson.pmf(r_support, expected_sup), r_support, expected_sup)\n",
    "\n",
    "                if poisson.pmf(r_support, expected_sup) < threshold:\n",
    "                    new_candidate = create_new_candidate(candidate, dim_bin, reevaluated_points)\n",
    "                    new_candidates.append(new_candidate)\n",
    "                    print(\"Length of new candidates after poisson\", len(new_candidates))\n",
    "    return new_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSignature:\n",
    "    def __init__(self, bins, assigned_points=[]):\n",
    "        self.bins = bins\n",
    "        self.bin_dict = {}\n",
    "        for _bin in bins:\n",
    "            self.bin_dict[_bin.id] = _bin\n",
    "        self.assigned_points = assigned_points\n",
    "        self.id = list([_bin.id for _bin in bins])\n",
    "        self.parent = False\n",
    "        \n",
    "    def get_support(self):\n",
    "        return len(self.assigned_points)\n",
    "    \n",
    "    def add_bin(self, _bin):\n",
    "        self.bins.append(_bin)\n",
    "        self.assigned_points.append(_bin.assigned_support)\n",
    "        self.id.append(_bin.index)\n",
    "        self.bin_dict[_bin.id] = _bin\n",
    "    \n",
    "    def reevaluate_assigned_points(self, _bin, current_dim):\n",
    "        evaluated_points = []\n",
    "        current_interval = _bin.interval\n",
    "        for point in self.assigned_points:\n",
    "            if point[current_dim] > current_interval.start and point[current_dim] <= current_interval.end:\n",
    "                evaluated_points.append(point)\n",
    "        return evaluated_points\n",
    "    \n",
    "    def get_means(self):\n",
    "        return np.average(np.array(self.assigned_points), axis = 0)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"NumBins: {len(self.bins)}, NumPoints: {len(self.assigned_points)}, Parent: {self.parent}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "candidate_list = []\n",
    "for _bin in new_bins[0]:\n",
    "    if _bin.marked:\n",
    "        candidate_list.append(PSignature([_bin], assigned_points=_bin.assigned_points))\n",
    "\n",
    "poisson_threshold = 1e-4\n",
    "for dim in range(1, len(data[0])):    \n",
    "    candidate_list = generate_candidate_list(candidate_list, new_bins[dim], poisson_threshold, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent_level items d0\n",
      "parent_level items d1\n",
      "parent_level items d2\n",
      "parent_level items d3\n",
      "parent_level items d4\n",
      "parent_level items d0 d1\n",
      "parent_level items d0 d2\n",
      "parent_level items d0 d3\n",
      "parent_level items d0 d4\n",
      "parent_level items d1 d2\n",
      "parent_level items d1 d3\n",
      "parent_level items d1 d4\n",
      "parent_level items d2 d3\n",
      "parent_level items d2 d4\n",
      "parent_level items d3 d4\n",
      "parent_level items d0 d1 d2\n",
      "parent_level items d0 d1 d3\n",
      "parent_level items d0 d1 d4\n",
      "parent_level items d0 d2 d3\n",
      "parent_level items d0 d2 d4\n",
      "parent_level items d0 d3 d4\n",
      "parent_level items d1 d2 d3\n",
      "parent_level items d1 d2 d4\n",
      "parent_level items d1 d3 d4\n",
      "parent_level items d2 d3 d4\n",
      "parent_level items d0 d1 d2 d3\n",
      "parent_level items d0 d1 d2 d4\n",
      "parent_level items d0 d1 d3 d4\n",
      "parent_level items d0 d2 d3 d4\n",
      "parent_level items d1 d2 d3 d4\n",
      "parent_level items d0 d1 d2 d3 d4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'d0': {'d0bd0b1': NumBins: 1, NumPoints: 582, Parent: True},\n",
       "  'd1': {'d1bd1b0': NumBins: 1, NumPoints: 598, Parent: True},\n",
       "  'd2': {'d2bd2b0': NumBins: 1, NumPoints: 592, Parent: True},\n",
       "  'd3': {'d3bd3b0': NumBins: 1, NumPoints: 586, Parent: True},\n",
       "  'd4': {'d4bd4b0': NumBins: 1, NumPoints: 593, Parent: True,\n",
       "   'd4bd4b8': NumBins: 1, NumPoints: 7, Parent: False}},\n",
       " 1: {'d0 d1': {'d0b1 d1b0': NumBins: 2, NumPoints: 579, Parent: True},\n",
       "  'd0 d2': {'d0b1 d2b0': NumBins: 2, NumPoints: 573, Parent: True},\n",
       "  'd0 d3': {'d0b1 d3b0': NumBins: 2, NumPoints: 567, Parent: True},\n",
       "  'd0 d4': {'d0b1 d4b0': NumBins: 2, NumPoints: 575, Parent: True},\n",
       "  'd1 d2': {'d1b0 d2b0': NumBins: 2, NumPoints: 589, Parent: True},\n",
       "  'd1 d3': {'d1b0 d3b0': NumBins: 2, NumPoints: 583, Parent: True},\n",
       "  'd1 d4': {'d1b0 d4b0': NumBins: 2, NumPoints: 590, Parent: True},\n",
       "  'd2 d3': {'d2b0 d3b0': NumBins: 2, NumPoints: 577, Parent: True},\n",
       "  'd2 d4': {'d2b0 d4b0': NumBins: 2, NumPoints: 584, Parent: True},\n",
       "  'd3 d4': {'d3b0 d4b0': NumBins: 2, NumPoints: 578, Parent: False}},\n",
       " 2: {'d0 d1 d2': {'d0b1 d1b0 d2b0': NumBins: 3, NumPoints: 570, Parent: True},\n",
       "  'd0 d1 d3': {'d0b1 d1b0 d3b0': NumBins: 3, NumPoints: 564, Parent: True},\n",
       "  'd0 d1 d4': {'d0b1 d1b0 d4b0': NumBins: 3, NumPoints: 572, Parent: True},\n",
       "  'd0 d2 d3': {'d0b1 d2b0 d3b0': NumBins: 3, NumPoints: 558, Parent: True},\n",
       "  'd0 d2 d4': {'d0b1 d2b0 d4b0': NumBins: 3, NumPoints: 566, Parent: True},\n",
       "  'd0 d3 d4': {'d0b1 d3b0 d4b0': NumBins: 3, NumPoints: 560, Parent: False},\n",
       "  'd1 d2 d3': {'d1b0 d2b0 d3b0': NumBins: 3, NumPoints: 574, Parent: True},\n",
       "  'd1 d2 d4': {'d1b0 d2b0 d4b0': NumBins: 3, NumPoints: 581, Parent: True},\n",
       "  'd1 d3 d4': {'d1b0 d3b0 d4b0': NumBins: 3, NumPoints: 575, Parent: False},\n",
       "  'd2 d3 d4': {'d2b0 d3b0 d4b0': NumBins: 3, NumPoints: 569, Parent: False}},\n",
       " 3: {'d0 d1 d2 d3': {'d0b1 d1b0 d2b0 d3b0': NumBins: 4, NumPoints: 555, Parent: True},\n",
       "  'd0 d1 d2 d4': {'d0b1 d1b0 d2b0 d4b0': NumBins: 4, NumPoints: 563, Parent: True},\n",
       "  'd0 d1 d3 d4': {'d0b1 d1b0 d3b0 d4b0': NumBins: 4, NumPoints: 557, Parent: False},\n",
       "  'd0 d2 d3 d4': {'d0b1 d2b0 d3b0 d4b0': NumBins: 4, NumPoints: 551, Parent: False},\n",
       "  'd1 d2 d3 d4': {'d1b0 d2b0 d3b0 d4b0': NumBins: 4, NumPoints: 566, Parent: False}},\n",
       " 4: {'d0 d1 d2 d3 d4': {'d0b1 d1b0 d2b0 d3b0 d4b0': NumBins: 5, NumPoints: 548, Parent: False}},\n",
       " 5: {}}"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_tree = {}\n",
    "candidate_tree_0 = {}\n",
    "for dim in range(0, len(data[0])):\n",
    "    candidate_dict = {}\n",
    "    for _bin in new_bins[dim]:\n",
    "        if _bin.marked:\n",
    "            candidate_dict[f'd{dim}b{_bin.id}'] = PSignature([_bin], assigned_points=_bin.assigned_points)\n",
    "    dim_id = 'd{}'.format(dim)\n",
    "    candidate_tree_0[dim_id] = candidate_dict\n",
    "candidate_tree[0] = candidate_tree_0\n",
    "\n",
    "def construct_new_level(parent_level_id, candidate_tree, threshold):\n",
    "    stop = False\n",
    "    while not stop:\n",
    "        parent_level = candidate_tree[parent_level_id]\n",
    "        current_candidates_ids = []\n",
    "        new_level_tree = {}\n",
    "        for (p1_id, p1_psigs) in parent_level.items():\n",
    "            print(\"parent_level items\", p1_id)\n",
    "            for (p2_id, p2_psigs) in parent_level.items():\n",
    "                l = len(p1_id)\n",
    "                if (p1_id[0:l-2] == p2_id[0:l-2] and p1_id[l-2:] != p2_id[l-2:]):\n",
    "                    current_candidate_id_list = [p2_id[l-2:]]\n",
    "                    current_candidate_id_list += p1_id.split(' ')\n",
    "                    current_candidate_id = ' '.join(sorted(current_candidate_id_list))\n",
    "                    if current_candidate_id not in current_candidates_ids:\n",
    "                        current_candidates_ids.append(current_candidate_id)\n",
    "                        res = construct_new_signatures(p1_psigs, p2_psigs, threshold)\n",
    "                        new_level_tree[current_candidate_id] = res\n",
    "        if not new_level_tree:\n",
    "            stop = True\n",
    "        candidate_tree[parent_level_id + 1] = new_level_tree\n",
    "        parent_level_id += 1\n",
    "    return candidate_tree\n",
    "                    \n",
    "def construct_new_signatures(p1_psigs, p2_psigs, threshold):\n",
    "    new_candidates = {}\n",
    "    for _, p1_psig in p1_psigs.items():\n",
    "        p1_psig_id = ' '.join(sorted(p1_psig.id))\n",
    "        l = len(p1_psig_id)\n",
    "        for _, p2_psig in p2_psigs.items():\n",
    "            p2_psig_id = ' '.join(sorted(p2_psig.id))\n",
    "            if (set(p1_psig.id[0:-1]) == set(p2_psig.id[0:-1])):\n",
    "                dim_bin_id = p2_psig.id[-1]\n",
    "                p12_psig_id = ' '.join(sorted([p1_psig_id, dim_bin_id]))\n",
    "                \n",
    "                dim_bin = p2_psig.bin_dict[dim_bin_id]\n",
    "                if dim_bin.marked:\n",
    "                    candidate = p1_psig\n",
    "                    expected_sup = candidate.get_support() * dim_bin.get_width()            \n",
    "                    reevaluated_points = candidate.reevaluate_assigned_points(dim_bin, dim_bin.dimension)\n",
    "                    r_support = len(reevaluated_points)\n",
    "                    if r_support == 0:\n",
    "                        continue\n",
    "#                     print('R support {0}, expected support {1}'.format(r_support, expected_sup))\n",
    "#                     print('Poisson distribution:', poisson.pmf(r_support, expected_sup), r_support, expected_sup)\n",
    "\n",
    "                    if poisson.pmf(r_support, expected_sup) < threshold:\n",
    "                        p1_psig.parent = True\n",
    "                        p2_psig.parent = True\n",
    "                        new_candidate = create_new_candidate(candidate, dim_bin, reevaluated_points)\n",
    "                        new_candidates[p12_psig_id] = new_candidate\n",
    "#                         print('{} + {} = {}'.format(p1_psig_id, p2_psig_id, p12_psig_id))\n",
    "#                         print(\"Length of new candidates after poisson\", len(new_candidates))\n",
    "\n",
    "    return new_candidates\n",
    "    \n",
    "    \n",
    "            \n",
    "\n",
    "# for (level_id, level) in candidate_tree.items():\n",
    "#     print('Level ', level_id)\n",
    "#     for (dim_id, psigs) in level.items():\n",
    "#         print(\"Dimension \", dim_id)\n",
    "#         for psig in psigs:\n",
    "#             print(psig.id)\n",
    "\n",
    "poisson_threshold = 1e-10\n",
    "ns = construct_new_level(0, candidate_tree, 1e-10)\n",
    "\n",
    "ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (level_id, level) in candidate_tree.items():\n",
    "    print('Level ', level_id)\n",
    "    for (dim_id, psigs) in level.items():\n",
    "        print(\"Dimension \", dim_id)\n",
    "        print(type(psigs))\n",
    "        for _, psig in psigs.items():\n",
    "            print(psig.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in candidate_list:\n",
    "    print(\"---\")\n",
    "    for b in c.bins:\n",
    "        print(b.index, b.dimension, len(b.assigned_points), b.marked)\n",
    "        print(b.interval)\n",
    "    print(len(c.assigned_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bin in new_bins[1]:\n",
    "    print(bin.marked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for candidate in candidate_list:\n",
    "    for _bin in candidate.bins:\n",
    "        print('start {0}, end {1}, length {2}'.format(_bin.interval.start, _bin.interval.end, _bin.interval.length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPoint:\n",
    "    def __init__(self, coords):\n",
    "        self.coords = coords\n",
    "        self.assigned_clusters = []\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        if len(other.coords) != len(self.coords):\n",
    "            return False\n",
    "        for i, x in enumerate(self.coords):\n",
    "            if x - other.coords[i] > 1e-9:\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in candidate_list:\n",
    "    print(c.get_means())\n",
    "\n",
    "inv_cov_cluster_dict = dict()\n",
    "\n",
    "for i,can in enumerate(candidate_list):   \n",
    "    cov = np.cov(np.array(can.assigned_points).T)\n",
    "    inv_covmat= np.linalg.inv(cov)\n",
    "    inv_cov_cluster_dict[i] = inv_covmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuzzy membership matrix\n",
    "#initialize matrix with datapoints in one column and found cluster (e.g. 1,2,3) in other column\n",
    "#initialize clusterpoints with a 1 at the matrix intersection\n",
    "matrix = np.zeros(dtype='float', shape=(len(data), len(candidate_list)))\n",
    "dps = []\n",
    "\n",
    "print(matrix.shape)\n",
    "\n",
    "cov_dat = np.cov(np.array(data).T)\n",
    "inv_covmat_dat= np.linalg.inv(cov_dat)\n",
    "\n",
    "for i, point in enumerate(data):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    data_point = DataPoint(point)\n",
    "    for j, candidate in enumerate(candidate_list):\n",
    "        candidate_data_points = [DataPoint(p) for p in candidate.assigned_points]\n",
    "        if data_point in candidate_data_points:\n",
    "            matrix[i][j] = 1\n",
    "            data_point.assigned_clusters.append(j)\n",
    "    fraction = 1 if len(data_point.assigned_clusters) == 0 else 1 / len(data_point.assigned_clusters) \n",
    "    for r in range(len(candidate_list)):\n",
    "        if matrix[i][r] == 1:\n",
    "            matrix[i][r] = fraction\n",
    "    #\"\"\"\n",
    "    if len(data_point.assigned_clusters) == 0:\n",
    "        closest = sys.maxsize\n",
    "        closest_candidate_idx = 0\n",
    "        for idx, c in enumerate(candidate_list):\n",
    "            mh_distance = distance.mahalanobis(data_point.coords, c.get_means(), inv_cov_cluster_dict[idx])\n",
    "            if mh_distance < closest:\n",
    "                closest = mh_distance\n",
    "                closest_candidate_idx = idx\n",
    "        data_point.assigned_clusters.append(closest_candidate_idx)\n",
    "        matrix[i][closest_candidate_idx] = 1\n",
    "    #\"\"\"\n",
    "    dps.append(data_point)\n",
    "                \n",
    "#compute mean of support set of cluster\n",
    "\n",
    "#compute the shortest mahalanobis distance(scipy.spatial.distance.mahalanobis) \n",
    "# of unassigned points to cluster core and assign\n",
    "\n",
    "# EM -> probably need to implement ourself\n",
    "means_before = np.array([c.get_means() for c in candidate_list])\n",
    "\n",
    "gmm = mixture.BayesianGaussianMixture(n_components=len(candidate_list), covariance_type='full').fit(matrix)\n",
    "\n",
    "#gmm2 = mixture.BayesianGaussianMixture(n_components=len(candidate_list), covariance_type='full').fit(data)\n",
    "\n",
    "result = gmm.predict(matrix)\n",
    "\n",
    "#result2 = gmm2.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_points = list()\n",
    "projected_cluster_dict = dict()\n",
    "for c in range(len(candidate_list)):\n",
    "    projected_cluster_dict[c] = []\n",
    "\n",
    "for assigned_cluster, p in list(zip(result, data)):\n",
    "    clustered_points.append((assigned_cluster,p))\n",
    "    if assigned_cluster in projected_cluster_dict:\n",
    "        projected_cluster_dict[assigned_cluster].append(p)\n",
    "\n",
    "means_after_bgm = list()        \n",
    "for pj in projected_cluster_dict.keys():\n",
    "    means_after_bgm.append(np.mean(np.array(projected_cluster_dict[pj]), axis = 0))\n",
    "\n",
    "\n",
    "    \n",
    "amount = 0    \n",
    "for pj in projected_cluster_dict.keys():\n",
    "    amount += len(projected_cluster_dict[pj])\n",
    "\n",
    "print(amount)\n",
    "    \n",
    "plt.scatter([x[0] for x in data], [y[1] for y in data], c=result, s=20)\n",
    "plt.scatter([x[0] for x in means_after_bgm], [y[1] for y in means_after_bgm], c=\"green\")\n",
    "plt.scatter([x[0] for x in means_before], [y[1] for y in means_before], c=\"red\")\n",
    "\n",
    "plt.show()\n",
    "print(means_after_bgm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_cov_dict = dict()\n",
    "\n",
    "for key in projected_cluster_dict.keys():   \n",
    "    cov = np.cov(np.array(projected_cluster_dict[key]).T)\n",
    "    inv_covmat= np.linalg.inv(cov)\n",
    "    inv_cov_dict[key] = inv_covmat\n",
    "    \n",
    "degree_of_freedom = (len(data[0])-1) *(len(data[1])-1)\n",
    "#degree_of_freedom = 10\n",
    "chi_crit = chi2.ppf(0.001, df=degree_of_freedom)\n",
    "\n",
    "noise_cluster_idx  = len(candidate_list)\n",
    "print(chi_crit)\n",
    "for i, c in enumerate(clustered_points):\n",
    "    cluster_mean = means_after_bgm[c[0]] \n",
    "    md = distance.mahalanobis(c[1],cluster_mean,inv_cov_dict[c[0]])\n",
    "    if md > chi_crit:\n",
    "        clustered_points[i] = (noise_cluster_idx,c[1])\n",
    "        print(md, clustered_points[i], chi_crit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter([x[1][0] for x in clustered_points], [y[1][1] for y in clustered_points], c = [z[0] for z in clustered_points])\n",
    "plt.scatter([x[0] for x in means_after_bgm], [y[1] for y in means_after_bgm], c=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(means_after_bgm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gmm.means_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
