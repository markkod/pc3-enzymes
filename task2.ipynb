{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of generate_kernels.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markkod/pc3-enzymes/blob/exploratory_da/task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VxEwG3kUGD08",
        "outputId": "3633c342-4561-4031-c5e5-22d42d24b6b9"
      },
      "source": [
        "#preinstalled version of pytorch has to be the same as the pre-compiled versions of the pytorch-geometric packages that we download later on.\r\n",
        "#versions might change quickly, so if you get a strange error later on, check the torch version of Google colab later on as follows:\r\n",
        "\r\n",
        "import torch\r\n",
        "torch.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.7.0+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-VN1RcQBUvc",
        "outputId": "e3b55c61-af2e-441e-a13e-ff3bf8fb028b"
      },
      "source": [
        "# Script to generate variations of the kernels yourself\n",
        "# https://ucloud.univie.ac.at/index.php/s/E3YKph0jkpbw8TN\n",
        "\n",
        "\n",
        "# #Download the TUDataset Repository with\n",
        "!git clone https://github.com/chrsmrrs/tudataset.git\n",
        "# #move this script to tudataset/tud_benchmark\n",
        "\n",
        "# #Install pytorch geometric: https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html\n",
        "# #Here is the gpu cuda installation, for the cpu version replace cu102 with cpu\n",
        "%pip --no-cache-dir install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "%pip --no-cache-dir install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "%pip --no-cache-dir install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "%pip --no-cache-dir install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
        "%pip --no-cache-dir install torch-geometric\n",
        "\n",
        "%pip --no-cache-dir install pybind11\n",
        "!sudo apt-get install libeigen3-dev\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'tudataset' already exists and is not an empty directory.\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
            "Collecting torch-scatter==latest+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.7.0/torch_scatter-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (11.9MB)\n",
            "\u001b[K     |████████████████████████████████| 11.9MB 34.3MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "  Found existing installation: torch-scatter 2.0.5\n",
            "    Uninstalling torch-scatter-2.0.5:\n",
            "      Successfully uninstalled torch-scatter-2.0.5\n",
            "Successfully installed torch-scatter-2.0.5\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
            "Collecting torch-sparse==latest+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.7.0/torch_sparse-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (24.3MB)\n",
            "\u001b[K     |████████████████████████████████| 24.3MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse==latest+cu101) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse==latest+cu101) (1.19.4)\n",
            "Installing collected packages: torch-sparse\n",
            "  Found existing installation: torch-sparse 0.6.8\n",
            "    Uninstalling torch-sparse-0.6.8:\n",
            "      Successfully uninstalled torch-sparse-0.6.8\n",
            "Successfully installed torch-sparse-0.6.8\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
            "Collecting torch-cluster==latest+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.7.0/torch_cluster-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (21.5MB)\n",
            "\u001b[K     |████████████████████████████████| 21.5MB 1.3MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "  Found existing installation: torch-cluster 1.5.8\n",
            "    Uninstalling torch-cluster-1.5.8:\n",
            "      Successfully uninstalled torch-cluster-1.5.8\n",
            "Successfully installed torch-cluster-1.5.8\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.7.0.html\n",
            "Collecting torch-spline-conv==latest+cu101\n",
            "\u001b[?25l  Downloading https://pytorch-geometric.com/whl/torch-1.7.0/torch_spline_conv-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl (6.4MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4MB 51.5MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "  Found existing installation: torch-spline-conv 1.2.0\n",
            "    Uninstalling torch-spline-conv-1.2.0:\n",
            "      Successfully uninstalled torch-spline-conv-1.2.0\n",
            "Successfully installed torch-spline-conv-1.2.0\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.6/dist-packages (1.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: ase in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (3.20.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.14)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.11.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.1.5)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (5.0.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.7.0+cu101)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.19.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (1.0.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from ase->torch-geometric) (3.2.2)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (51.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (0.6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->ase->torch-geometric) (0.10.0)\n",
            "Requirement already satisfied: pybind11 in /usr/local/lib/python3.6/dist-packages (2.6.1)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libeigen3-dev is already the newest version (3.3.4-4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXKQ-3OcCzMK",
        "outputId": "29d4fb01-dde5-4ce2-ba13-72b001090ada"
      },
      "source": [
        "%cd ..\n",
        "%cd /content/tudataset/tud_benchmark/kernel_baselines/\n",
        "! ls\n",
        "! g++ -I /usr/include/eigen3 -O3 -shared -std=c++11 -fPIC `python3 -m pybind11 --includes`  kernel_baselines.cpp src/*cpp -o ../kernel_baselines`python3-config --extension-suffix`\n",
        "%cd .."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/\n",
            "/content/tudataset/tud_benchmark/kernel_baselines\n",
            "kernel_baselines.cpp  src\n",
            "/content/tudataset/tud_benchmark\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjESzy3qMo8C",
        "outputId": "851d435f-aa9e-4e26-e98b-9e97f56b0a1c"
      },
      "source": [
        "!ls -al /usr/local/cuda"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lrwxrwxrwx 1 root root 9 Dec 21 17:24 /usr/local/cuda -> cuda-10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8HOSZDPBbNV"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import kernel_baselines as kb\n",
        "import auxiliarymethods\n",
        "from auxiliarymethods import datasets as dp\n",
        "from scipy.sparse import save_npz\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pq17mvdBdym"
      },
      "source": [
        "def setup_directory(dir_name, verbose=False):\n",
        "    \"\"\"\n",
        "    Setup directory in case it does not exist\n",
        "    Parameters:\n",
        "    -------------\n",
        "    dir_name: str, path + name to directory\n",
        "    verbose: bool, indicates whether directory creation should be printed or not.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(dir_name):\n",
        "        try:\n",
        "            os.makedirs(dir_name)\n",
        "            if verbose:\n",
        "                print(\"Created Directory: {}\".format(dir_name))\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(\n",
        "                \"Could not create directory: {}\\n {}\".format(dir_name, e))\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoWVqxq5B8RW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757b3250-e489-4329-deec-8535a5d4a2b8"
      },
      "source": [
        "use_edge_labels = False\n",
        "for USE_LABELS in [True, False]:# Except IMDB-BINARY\n",
        "    for dataset, use_labels in [[\"ENZYMES\", USE_LABELS]]:\n",
        "        if use_labels:\n",
        "            base_path = os.path.join(\"kernels\",\"node_labels\")\n",
        "        else:\n",
        "            base_path = os.path.join(\"kernels\",\"without_labels\")\n",
        "        setup_directory(base_path)\n",
        "        print(\"Start processing data set \", dataset)\n",
        "        # Download dataset.\n",
        "        classes = dp.get_dataset(dataset)\n",
        "        # *Weisfeihler-Lehman*\n",
        "        print(\"Start computing Weisfeihler-Lehman gram matrix and vector representations\")\n",
        "        iterations = 6\n",
        "        #0 taking just the nodelabels themselves into account; \n",
        "        #1 considers nearest-neighbours, 2 one layer deeper and so on\n",
        "        for i in range(1, iterations):\n",
        "            print(\"Start iteration \", i)\n",
        "            #Gram Matrix for the Weisfeiler-Lehman subtree kernel\n",
        "            gram_matrix_wl = kb.compute_wl_1_dense(dataset, i, use_labels, use_edge_labels)\n",
        "            np.savetxt(os.path.join(base_path,f\"{dataset}_gram_matrix_wl{i}.csv\"),\n",
        "                    gram_matrix_wl,\n",
        "                    delimiter=\";\")\n",
        "            #Sparse Vectors for the Weisfeiler-Lehmann subtree kernel\n",
        "            vectors_wl = kb.compute_wl_1_sparse(dataset, i, use_labels, use_edge_labels)\n",
        "            save_npz(os.path.join(base_path,f\"{dataset}_vectors_wl{i}.npz\"),\n",
        "                    vectors_wl, compressed=True)\n",
        "\n",
        "        # *Graphlet kernel*\n",
        "        print(\"Start computing Graphlet gram matrix\")\n",
        "\n",
        "        #Gram Matrix for the Graphlet kernel\n",
        "        gram_matrix_graphlet= kb.compute_graphlet_dense(dataset, use_labels, use_edge_labels)\n",
        "        np.savetxt(os.path.join(base_path,f\"{dataset}_gram_matrix_graphlet.csv\"),\n",
        "                gram_matrix_graphlet,\n",
        "                delimiter=\";\")\n",
        "\n",
        "        print(\"Start computing Graphlet vector representation\")\n",
        "        #Sparse Vectors for the Graphlet kernel\n",
        "        vectors_graphlet = kb.compute_graphlet_sparse(dataset, use_labels, use_edge_labels)\n",
        "        save_npz(os.path.join(base_path,f\"{dataset}_vectors_graphlet.npz\"),\n",
        "                vectors_graphlet, compressed=True)\n",
        "\n",
        "\n",
        "        print(\"Start computing Shortest path gram matrix\")\n",
        "\n",
        "        #Gram Matrix for the Shortest path kernel\n",
        "        gram_matrix_shortestpath = kb.compute_shortestpath_dense(dataset, use_labels)\n",
        "        np.savetxt(os.path.join(base_path,f\"{dataset}_gram_matrix_shortestpath.csv\"),\n",
        "                gram_matrix_shortestpath,\n",
        "                delimiter=\";\")\n",
        "\n",
        "        print(\"Start computing Shortest path vector representation\")\n",
        "\n",
        "        #Sparse Vectors for the Shortest path kernel\n",
        "        vectors_shortestpath = kb.compute_shortestpath_sparse(dataset, use_labels)\n",
        "        save_npz(os.path.join(base_path,f\"{dataset}_vectors_shortestpath.npz\"),\n",
        "                vectors_shortestpath, compressed=True)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start processing data set  ENZYMES\n",
            "Start computing Weisfeihler-Lehman gram matrix and vector representations\n",
            "Start iteration  1\n",
            "Start iteration  2\n",
            "Start iteration  3\n",
            "Start iteration  4\n",
            "Start iteration  5\n",
            "Start computing Graphlet gram matrix\n",
            "Start computing Graphlet vector representation\n",
            "Start computing Shortest path gram matrix\n",
            "Start computing Shortest path vector representation\n",
            "Start processing data set  ENZYMES\n",
            "Start computing Weisfeihler-Lehman gram matrix and vector representations\n",
            "Start iteration  1\n",
            "Start iteration  2\n",
            "Start iteration  3\n",
            "Start iteration  4\n",
            "Start iteration  5\n",
            "Start computing Graphlet gram matrix\n",
            "Start computing Graphlet vector representation\n",
            "Start computing Shortest path gram matrix\n",
            "Start computing Shortest path vector representation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QCfal8eONNe"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df1 = pd.read_csv('/content/tudataset/tud_benchmark/kernels/node_labels/ENZYMES_gram_matrix_wl1.csv', sep=';', header=None)\n",
        "df2 = pd.read_csv('/content/tudataset/tud_benchmark/kernels/node_labels/ENZYMES_gram_matrix_wl2.csv', sep=';', header=None)\n",
        "df3 = pd.read_csv('/content/tudataset/tud_benchmark/kernels/node_labels/ENZYMES_gram_matrix_wl3.csv', sep=';', header=None)\n",
        "df4 = pd.read_csv('/content/tudataset/tud_benchmark/kernels/node_labels/ENZYMES_gram_matrix_wl4.csv', sep=';', header=None)\n",
        "df5 = pd.read_csv('/content/tudataset/tud_benchmark/kernels/node_labels/ENZYMES_gram_matrix_wl5.csv', sep=';', header=None)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z7c_D4yQuMX",
        "outputId": "1eadea59-7c5d-4542-9188-f5b69ef4d320"
      },
      "source": [
        "from auxiliarymethods import auxiliary_methods as aux\n",
        "from auxiliarymethods import kernel_evaluation as ke\n",
        "\n",
        "dfs = [df1, df2, df3, df3, df4, df5]\n",
        "classes = dp.get_dataset('ENZYMES')\n",
        "all_matrices = []\n",
        "for df in dfs:\n",
        "  df_np = df.to_numpy()\n",
        "  gm = aux.normalize_gram_matrix(df_np)\n",
        "  all_matrices.append(gm)\n",
        "\n",
        "accuracy, std_10, std_100 = ke.kernel_svm_evaluation(all_matrices, classes, num_repetitions=10, all_std=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlO05E6lSnpY"
      },
      "source": [
        "sp = pd.read_csv('/content/tudataset/tud_benchmark/kernels/node_labels/ENZYMES_gram_matrix_shortestpath.csv', sep=';', header=None).to_numpy()\n",
        "graphlet = pd.read_csv('/content/tudataset/tud_benchmark/kernels/node_labels/ENZYMES_gram_matrix_graphlet.csv', sep=';', header=None).to_numpy()\n",
        "\n",
        "baseline_kernels = {\n",
        "    'WL': dfs,\n",
        "    'SP': [sp],\n",
        "    'GRAPHLET': [graphlet]\n",
        "}\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss61LI-cTQ6e"
      },
      "source": [
        "# TODO: ASK DURING QA SESSION\n",
        "#accuracy_sp, std_10_sp, std_100_sp = ke.kernel_svm_evaluation(baseline_kernels['SP'], classes, num_repetitions=10, all_std=True)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqQWGjUmTVWB"
      },
      "source": [
        "#accuracy_gl, std_10_gl, std_100_gl = ke.kernel_svm_evaluation(baseline_kernels['GRAPHLET'], classes, num_repetitions=10, all_std=True)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkt9ZBeKTzlB"
      },
      "source": [
        "def load_data():\n",
        "  result = {}\n",
        "  extensions = ['csv', 'npz']\n",
        "  types = ['gram_matrix', 'vectors']\n",
        "  algos = ['wl1', 'wl2', 'wl3', 'wl4', 'wl5', 'shortestpath', 'graphlet']\n",
        "  base_name = '/content/tudataset/tud_benchmark/kernels/node_labels/ENZYMES_{0}_{1}.{2}'\n",
        "  for t, e in zip(types, extensions):\n",
        "    for a in algos:\n",
        "      file_name = base_name.format(t, a, e)\n",
        "      if e == 'csv':\n",
        "        f = np.loadtxt(file_name, delimiter=';')\n",
        "      else:\n",
        "        f = np.load(file_name)\n",
        "      key = a + '_' + t\n",
        "      result[key] = f\n",
        "  return result\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lpO07rCTVx8"
      },
      "source": [
        "data = load_data()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfoeCGcmYw6m",
        "outputId": "981b6be0-21dc-4b93-faae-8861d7fcd734"
      },
      "source": [
        "data"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'graphlet_gram_matrix': array([[11177.,  6596.,  4333., ..., 11081., 13908., 10164.],\n",
              "        [ 6596.,  4879.,  3005., ...,  4882.,  5786.,  3278.],\n",
              "        [ 4333.,  3005.,  2132., ...,  2814.,  3236.,  1674.],\n",
              "        ...,\n",
              "        [11081.,  4882.,  2814., ..., 20397., 22060., 18291.],\n",
              "        [13908.,  5786.,  3236., ..., 22060., 28850., 24883.],\n",
              "        [10164.,  3278.,  1674., ..., 18291., 24883., 22359.]]),\n",
              " 'graphlet_vectors': <numpy.lib.npyio.NpzFile at 0x7f5dee6cc128>,\n",
              " 'shortestpath_gram_matrix': array([[ 73924.,  30546.,  35514., ...,  79166., 117398.,  96672.],\n",
              "        [ 30546.,  15744.,  16882., ...,  36894.,  56388.,  46384.],\n",
              "        [ 35514.,  16882.,  21316., ...,  33136.,  54940.,  44536.],\n",
              "        ...,\n",
              "        [ 79166.,  36894.,  33136., ..., 316796., 227164., 199264.],\n",
              "        [117398.,  56388.,  54940., ..., 227164., 337252., 289668.],\n",
              "        [ 96672.,  46384.,  44536., ..., 199264., 289668., 256200.]]),\n",
              " 'shortestpath_vectors': <numpy.lib.npyio.NpzFile at 0x7f5dedeccf28>,\n",
              " 'wl1_gram_matrix': array([[ 874.,  502.,  585., ...,  973.,  998.,  935.],\n",
              "        [ 502.,  360.,  362., ...,  591.,  615.,  580.],\n",
              "        [ 585.,  362.,  476., ...,  587.,  678.,  636.],\n",
              "        ...,\n",
              "        [ 973.,  591.,  587., ..., 1930., 1417., 1351.],\n",
              "        [ 998.,  615.,  678., ..., 1417., 1488., 1431.],\n",
              "        [ 935.,  580.,  636., ..., 1351., 1431., 1432.]]),\n",
              " 'wl1_vectors': <numpy.lib.npyio.NpzFile at 0x7f5dee7ad6d8>,\n",
              " 'wl2_gram_matrix': array([[ 919.,  502.,  585., ...,  973.,  998.,  959.],\n",
              "        [ 502.,  385.,  362., ...,  591.,  615.,  595.],\n",
              "        [ 585.,  362.,  511., ...,  587.,  678.,  655.],\n",
              "        ...,\n",
              "        [ 973.,  591.,  587., ..., 1999., 1417., 1369.],\n",
              "        [ 998.,  615.,  678., ..., 1417., 1545., 1475.],\n",
              "        [ 959.,  595.,  655., ..., 1369., 1475., 1546.]]),\n",
              " 'wl2_vectors': <numpy.lib.npyio.NpzFile at 0x7f5dedecc828>,\n",
              " 'wl3_gram_matrix': array([[ 964.,  502.,  585., ...,  973.,  998.,  983.],\n",
              "        [ 502.,  408.,  362., ...,  591.,  615.,  610.],\n",
              "        [ 585.,  362.,  546., ...,  587.,  678.,  674.],\n",
              "        ...,\n",
              "        [ 973.,  591.,  587., ..., 2054., 1417., 1387.],\n",
              "        [ 998.,  615.,  678., ..., 1417., 1596., 1501.],\n",
              "        [ 983.,  610.,  674., ..., 1387., 1501., 1652.]]),\n",
              " 'wl3_vectors': <numpy.lib.npyio.NpzFile at 0x7f5dedecc9e8>,\n",
              " 'wl4_gram_matrix': array([[1009.,  502.,  585., ...,  973.,  998., 1007.],\n",
              "        [ 502.,  431.,  362., ...,  591.,  615.,  625.],\n",
              "        [ 585.,  362.,  581., ...,  587.,  678.,  693.],\n",
              "        ...,\n",
              "        [ 973.,  591.,  587., ..., 2109., 1417., 1405.],\n",
              "        [ 998.,  615.,  678., ..., 1417., 1647., 1527.],\n",
              "        [1007.,  625.,  693., ..., 1405., 1527., 1758.]]),\n",
              " 'wl4_vectors': <numpy.lib.npyio.NpzFile at 0x7f5dedeccba8>,\n",
              " 'wl5_gram_matrix': array([[1054.,  502.,  585., ...,  973.,  998., 1031.],\n",
              "        [ 502.,  454.,  362., ...,  591.,  615.,  640.],\n",
              "        [ 585.,  362.,  616., ...,  587.,  678.,  712.],\n",
              "        ...,\n",
              "        [ 973.,  591.,  587., ..., 2164., 1417., 1423.],\n",
              "        [ 998.,  615.,  678., ..., 1417., 1698., 1553.],\n",
              "        [1031.,  640.,  712., ..., 1423., 1553., 1864.]]),\n",
              " 'wl5_vectors': <numpy.lib.npyio.NpzFile at 0x7f5dedeccd68>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "nnky-P2CZc5o",
        "outputId": "1fa839b8-cdad-457f-d552-5d68f59bdf1c"
      },
      "source": [
        "def find_keys_with_condition(data, cond):\n",
        "    return list(filter(lambda x: cond in x, data.keys()))\n",
        "\n",
        "def all_kernel_svm_eval(data):\n",
        "  matching_keys = find_keys_with_condition(data, 'vectors')\n",
        "  all_matrices = []\n",
        "  for key in matching_keys:\n",
        "    if 'wl' in key:\n",
        "  for df in dfs:\n",
        "    df_np = df.to_numpy()\n",
        "    gm = aux.normalize_gram_matrix(df_np)\n",
        "    all_matrices.append(gm)\n",
        "\n",
        "accuracy, std_10, std_100 = ke.kernel_svm_evaluation(all_matrices, classes, num_repetitions=10, all_std=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-d95865999936>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    for df in dfs:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    }
  ]
}